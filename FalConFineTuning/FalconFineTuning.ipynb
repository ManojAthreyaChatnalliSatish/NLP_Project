{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUgeinaZyxVV"
      },
      "source": [
        "## Installing Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNnkgBq7Q3EU",
        "outputId": "b0c8b0df-7eb4-4c3e-dba7-134a9df77be7"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q datasets bitsandbytes einops wandb"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the Data Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KJmHtRksTUmv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('dataforLLM.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "LS5JzMKz5RP7",
        "outputId": "958ba1c3-6ed0-4b0d-8d6a-be11521fc5cb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>merged_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>why does it look like someone spit on my food?...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>it'd mcdonalds. it is what it is as far as the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>made a mobile order got to the speaker and che...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>my mc. crispy chicken sandwich was customer se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>i repeat my order 3 times in the drive thru, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                      merged_review\n",
              "0           0  why does it look like someone spit on my food?...\n",
              "1           1  it'd mcdonalds. it is what it is as far as the...\n",
              "2           2  made a mobile order got to the speaker and che...\n",
              "3           3  my mc. crispy chicken sandwich was customer se...\n",
              "4           4  i repeat my order 3 times in the drive thru, a..."
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "le1r4sDSWjba"
      },
      "outputs": [],
      "source": [
        "df.drop(['Unnamed: 0'], axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "X8ao6pHH5hsY",
        "outputId": "911e6efc-2bde-455b-d5d9-df4131176964"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>merged_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>why does it look like someone spit on my food?...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it'd mcdonalds. it is what it is as far as the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>made a mobile order got to the speaker and che...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>my mc. crispy chicken sandwich was customer se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i repeat my order 3 times in the drive thru, a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       merged_review\n",
              "0  why does it look like someone spit on my food?...\n",
              "1  it'd mcdonalds. it is what it is as far as the...\n",
              "2  made a mobile order got to the speaker and che...\n",
              "3  my mc. crispy chicken sandwich was customer se...\n",
              "4  i repeat my order 3 times in the drive thru, a..."
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here the dataset is built in a way to facilitate to fit the instruct model which we will have:\n",
        "the structure is question->:answer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are spliting the dataset here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sYD0jpyYWr4D"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(df, test_size=0.05, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "COY2-JxgXfB-",
        "outputId": "35fc644f-1de3-4a89-f7d8-02305413097a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>merged_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4877</th>\n",
              "      <td>good-&gt;:positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4032</th>\n",
              "      <td>excellent-&gt;:positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22739</th>\n",
              "      <td>curbside is a joke waited for 15 mins, should ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9090</th>\n",
              "      <td>nice accurate and quick service. thanks!-&gt;:pos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9279</th>\n",
              "      <td>average service and food, long wait times at n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18809</th>\n",
              "      <td>poor-&gt;:negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17950</th>\n",
              "      <td>neutral-&gt;:negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19386</th>\n",
              "      <td>very modern mcdonald's. to go order pickup sti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13875</th>\n",
              "      <td>if there is any mcdonalds world wide that dese...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14359</th>\n",
              "      <td>fast, got my 2 grape jellies!-&gt;:positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           merged_review\n",
              "4877                                     good->:positive\n",
              "4032                                excellent->:positive\n",
              "22739  curbside is a joke waited for 15 mins, should ...\n",
              "9090   nice accurate and quick service. thanks!->:pos...\n",
              "9279   average service and food, long wait times at n...\n",
              "18809                                    poor->:negative\n",
              "17950                                 neutral->:negative\n",
              "19386  very modern mcdonald's. to go order pickup sti...\n",
              "13875  if there is any mcdonalds world wide that dese...\n",
              "14359           fast, got my 2 grape jellies!->:positive"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "i_MzevjwcAjZ",
        "outputId": "52c3e313-b616-4e50-e8cc-aebf2af0ebb7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>merged_review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>29153</th>\n",
              "      <td>this review is about the staff at this locatio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30570</th>\n",
              "      <td>worst experience ever. customer service is the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23381</th>\n",
              "      <td>disappointing. mcdonalds is my standby for fas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23825</th>\n",
              "      <td>we went here for breakfast. the place could ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1398</th>\n",
              "      <td>good-&gt;:positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8639</th>\n",
              "      <td>they forget everything and charge you for it-&gt;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20984</th>\n",
              "      <td>good-&gt;:positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6293</th>\n",
              "      <td>soooo slow!!!! came here to try and get a quic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9845</th>\n",
              "      <td>month-&gt;:negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22365</th>\n",
              "      <td>not from florida..but every mcdonald's i visit...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           merged_review\n",
              "29153  this review is about the staff at this locatio...\n",
              "30570  worst experience ever. customer service is the...\n",
              "23381  disappointing. mcdonalds is my standby for fas...\n",
              "23825  we went here for breakfast. the place could ha...\n",
              "1398                                     good->:positive\n",
              "8639   they forget everything and charge you for it->...\n",
              "20984                                    good->:positive\n",
              "6293   soooo slow!!!! came here to try and get a quic...\n",
              "9845                                    month->:negative\n",
              "22365  not from florida..but every mcdonald's i visit..."
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1630"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df.to_csv('test.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are creating a dataset iterator here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sOHpHNK9XngF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/manoj/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset,DatasetDict\n",
        "train_dataset_dict = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(train_df),\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRkdY4KPax5a",
        "outputId": "0da9d459-63a9-4d6c-cb5c-0407a2ce78ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['merged_review', '__index_level_0__'],\n",
              "        num_rows: 30970\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjOMoSbGSxx9"
      },
      "source": [
        "## Loading the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AjB0WAqFSzlD"
      },
      "source": [
        "Loading [Falcon 7B model](https://huggingface.co/tiiuae/falcon-7b), quantizing it in 4bit and attaching LoRA adapters on it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "47ac2264e3e74ca5bff02fd6e59e613c",
            "c84cabbd43334acc8cfe492cfe6cb20c",
            "cfb47a914f83422a96a71f725a059111",
            "14da63396d4d47dfa1d796a46fdd65bf",
            "9efe35cdc628424fb1eb7d916fdf6911",
            "8d253d6a1fb842f7ac7a95b61edd4e9d",
            "bfa3092b4f0c409eb98cbccc5ef15bc9",
            "e879736dc46d477c8fc400e0363ab99f",
            "81d1e15e894e4ddc952399e38ef41971",
            "857765cb729d436e85d29f34cdd288ce",
            "a53e6cb6f3354fbd84ff498ecf385e7e"
          ]
        },
        "id": "ZwXZbQ2dSwzI",
        "outputId": "4acf4eae-8e4c-488d-f122-670643fabca8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 8/8 [00:10<00:00,  1.29s/it]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n",
        "\n",
        "model_name = \"ybelkada/falcon-7b-sharded-bf16\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model.config.use_cache = False"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xNqIYtQcUBSm"
      },
      "source": [
        "Loding the tokenizer for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XDS2yYmlUAD6"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r98gsX5Z2Cx_"
      },
      "source": [
        "## Base model predicting before finetuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5F7ZvX71-0W",
        "outputId": "2562d936-0956-4b0c-958c-1268d0b14774"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: “no restrooms, no seats to eat, no stars” ->:\n",
            "“no restrooms, no seats to eat, no stars, no service”\n",
            "“It's a very good place. The staff is very friendly and they make it worth the money”\n",
            "Result: good ->: [url][/url] and here is the \"problem\": when we try to run the game from the emulator (we have tried to run the game in several emulators like \"nes emulator 0.85\", \"nes emulator 0.86.2 beta4\", \"nes emulator 0.85 beta 2\", etc, we get the following errors from the emulator: - we tried to run the game with all the options we can configure in the emulator (like in \"nes emulator 0.85 beta\", we set \"screen ratio\" to \"full screen\", we have set \"screen refresh rate\" to 60hz (we can set up to 70hz), etc. the screen is not full and it is \"jumping\" a lot (it seems like a screen refresh rate is not set to 60hz and \"it jumps\" to 30\n",
            "Result: the nastiest mcdonald's i have ever been in!! ->:\n",
            "(Source: f-l-a-t-t-a-l-e-s)\n",
            "I have a new love for the beach! I love swimming in the ocean and just sitting and listening to the waves roll up to the beach.\n",
            "It’s so weird to see this picture and remember that this was taken 5 days ago and the water is now 5 ft up the beach!\n",
            "(Source: kristyisabitch)\n",
            "“The only time we have to live life in all its fullness, its only when your heart is broken” – John Mayer\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "\n",
        "sequences = pipeline(\n",
        "   [\"“no restrooms, no seats to eat, no stars” ->:\",\"good ->:\",\"the nastiest mcdonald's i have ever been in!! ->:\"],\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq[0]['generated_text']}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see from above code, the pre-trained model is giving some random results which it learnt from the internet. We have to fine-tune the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the below code block we are utilizing Parameter efficient fine tuning library to aid us wih fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dQdvjTYTT1vQ"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"query_key_value\",\n",
        "        \"dense\",\n",
        "        \"dense_h_to_4h\",\n",
        "        \"dense_4h_to_h\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "OCFTvGW6aspE"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "outputDir = \"./results\"\n",
        "eval_accumulation_steps = 1\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=outputDir,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=10,\n",
        "    logging_steps=1,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=50,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3t6b2TkcJwy"
      },
      "source": [
        "Then finally pass everthing to the trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "01f23cfe961646ca990bf09cf94e24ea",
            "963e9011456a4bdcba2c1e0f62089bd0",
            "0f287323a0344153a98637c83037a8c1",
            "52c23e0c52fe422a81aae0c9e1542ebc",
            "d1ce51cc3c6f490a857f47401883117c",
            "82a5a9a96f63491992c83e723da52e19",
            "9d5096af08a54ddd9f3732ebb3a034ed",
            "65e4ed5820f141b7836713789ac0d3d0",
            "802bf15dca8b4f69a1cbd125e0e2e586",
            "838c7d9206104eebb471c4a8f72ff894",
            "cf712d8d84d244dfbdeeb7d6ae6014e3"
          ]
        },
        "id": "TNeOBgZeTl2H",
        "outputId": "1578a9d7-26c8-4b32-89ef-6455d9d6cc20"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/manoj/.local/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
            "  warnings.warn(\n",
            "Map: 100%|██████████| 30970/30970 [00:00<00:00, 61914.46 examples/s]\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset_dict['train'],\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"merged_review\",\n",
        "    max_seq_length=512,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWplqqDjb3sS"
      },
      "source": [
        "We will also pre-process the model by upcasting the layer norms in float 32 for more stable training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7OyIvEx7b1GT"
      },
      "outputs": [],
      "source": [
        "for name, module in trainer.model.named_modules():\n",
        "    if \"norm\" in name:\n",
        "        module = module.to(torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JApkSrCcL3O"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JjvisllacNZM"
      },
      "source": [
        "Now let's train the model.CALLING `trainer.train()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_kbS7nRxcMt7",
        "outputId": "7d4c04aa-1330-4c00-c76c-91c2cc629d4e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmanojcsathreya\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/manoj/Desktop/NLP/wandb/run-20231130_194954-atdyvujz</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/manojcsathreya/huggingface/runs/atdyvujz' target=\"_blank\">astral-oath-13</a></strong> to <a href='https://wandb.ai/manojcsathreya/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/manojcsathreya/huggingface' target=\"_blank\">https://wandb.ai/manojcsathreya/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/manojcsathreya/huggingface/runs/atdyvujz' target=\"_blank\">https://wandb.ai/manojcsathreya/huggingface/runs/atdyvujz</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "The current implementation of Falcon calls `torch.scaled_dot_product_attention` directly, this will be deprecated in the future in favor of the `BetterTransformer` API. Please install the latest optimum library with `pip install -U optimum` and call `model.to_bettertransformer()` to benefit from `torch.scaled_dot_product_attention` and future performance optimizations.\n",
            "  2%|▏         | 1/50 [00:01<01:34,  1.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.0028, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 2/50 [00:02<00:51,  1.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.0889, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 3/50 [00:02<00:37,  1.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.539, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 4/50 [00:03<00:30,  1.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.0936, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 5/50 [00:03<00:26,  1.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.0216, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 6/50 [00:04<00:24,  1.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.1341, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 7/50 [00:04<00:22,  1.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.3309, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 8/50 [00:05<00:20,  2.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.2059, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 9/50 [00:05<00:19,  2.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.9408, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 10/50 [00:06<00:18,  2.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.8298, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 11/50 [00:09<00:49,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.9967, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 12/50 [00:09<00:38,  1.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.629, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 13/50 [00:10<00:31,  1.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.4977, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|██▊       | 14/50 [00:10<00:25,  1.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.2979, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 15/50 [00:10<00:22,  1.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.3466, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 16/50 [00:11<00:19,  1.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.9415, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|███▍      | 17/50 [00:11<00:17,  1.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.8541, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 18/50 [00:12<00:16,  1.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.3422, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 19/50 [00:12<00:14,  2.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.6921, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 20/50 [00:13<00:14,  2.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.1078, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 21/50 [00:15<00:30,  1.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.065, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 22/50 [00:15<00:24,  1.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.871, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 46%|████▌     | 23/50 [00:16<00:19,  1.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.8499, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 24/50 [00:16<00:16,  1.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.6428, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 25/50 [00:17<00:14,  1.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.4813, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 26/50 [00:17<00:12,  1.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.7179, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 54%|█████▍    | 27/50 [00:18<00:11,  1.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.5369, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|█████▌    | 28/50 [00:18<00:10,  2.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.5671, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 58%|█████▊    | 29/50 [00:18<00:09,  2.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.604, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 30/50 [00:19<00:09,  2.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.5468, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|██████▏   | 31/50 [00:21<00:19,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.0788, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 32/50 [00:22<00:15,  1.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.8037, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 66%|██████▌   | 33/50 [00:22<00:12,  1.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.0116, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 68%|██████▊   | 34/50 [00:23<00:10,  1.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.5272, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 35/50 [00:23<00:08,  1.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.2237, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 36/50 [00:23<00:07,  1.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 2.0364, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▍  | 37/50 [00:24<00:06,  1.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.9987, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 76%|███████▌  | 38/50 [00:24<00:05,  2.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.7327, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|███████▊  | 39/50 [00:25<00:05,  2.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3635, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 40/50 [00:25<00:04,  2.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.672, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%|████████▏ | 41/50 [00:27<00:09,  1.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.799, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 42/50 [00:28<00:06,  1.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.1661, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 43/50 [00:28<00:05,  1.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 4.2681, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%|████████▊ | 44/50 [00:29<00:03,  1.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.4958, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 45/50 [00:29<00:02,  1.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.5488, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|█████████▏| 46/50 [00:30<00:02,  1.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.8142, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|█████████▍| 47/50 [00:30<00:01,  1.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.052, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 96%|█████████▌| 48/50 [00:30<00:00,  2.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0022, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 49/50 [00:31<00:00,  2.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0066, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:31<00:00,  2.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.0005, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [00:34<00:00,  1.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 36.165, 'train_samples_per_second': 1.383, 'train_steps_per_second': 1.383, 'train_loss': 2.687549252529861, 'epoch': 0.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=50, training_loss=2.687549252529861, metrics={'train_runtime': 36.165, 'train_samples_per_second': 1.383, 'train_steps_per_second': 1.383, 'train_loss': 2.687549252529861, 'epoch': 0.0})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sticked on to the best described parametes and fine tuned the model for 50 epochs. We can see the loss is gradually decreasing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "E2tFZUKII4gL"
      },
      "outputs": [],
      "source": [
        "lst_test_data = list(test_df['merged_review'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXFR60GMvkZr",
        "outputId": "ef7e0ec9-d418-47ab-e240-63b56b66b49f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1630"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(lst_test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "XizZTnNivpFx"
      },
      "outputs": [],
      "source": [
        "lst_test_data_short = lst_test_data[:1000]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inferencing takes a lot of time. So we are limiting to first 1000 reviews. We are also persisting the inference results to save the results as we are prone to single point of failure "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrLHmLytLFhQ",
        "outputId": "e45c26ac-4278-4320-e696-630bc880d378"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 173, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 103, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 113, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 105, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 320, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 124, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 120, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 382, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 143, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "res = []\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "sequences = pipeline(\n",
        "    lst_test_data_short[:250],\n",
        "    max_length=100, \n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "for ix,seq in enumerate(sequences):\n",
        "    res.append([ix, seq[0]['generated_text'].split('->:')[0] +  \"->\" +seq[0]['generated_text'].split('->:')[1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 136, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 143, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 229, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 338, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 176, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 110, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 127, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 107, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 122, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 171, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 100, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 126, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "res = []\n",
        "sequences = pipeline(\n",
        "    lst_test_data_short[500:751],\n",
        "    max_length=100,  #200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "for ix,seq in enumerate(sequences):\n",
        "    res.append([ix, seq[0]['generated_text'].split('->:')[0] +  \"->\" +seq[0]['generated_text'].split('->:')[1]])\n",
        "\n",
        "with open('500to750.csv', 'w') as f:\n",
        "    for i in res:\n",
        "        f.write(str(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 232, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 154, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 101, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 106, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 225, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 132, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 129, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 116, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 141, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "res = []\n",
        "sequences = pipeline(\n",
        "    lst_test_data_short[250:500],\n",
        "    max_length=100,  #200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "for ix,seq in enumerate(sequences):\n",
        "    res.append([ix, seq[0]['generated_text'].split('->:')[0] +  \"->\" +seq[0]['generated_text'].split('->:')[1]])\n",
        "\n",
        "with open('250to500.csv', 'w') as f:\n",
        "    for i in res:\n",
        "        f.write(str(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUQFjhUcDSZy",
        "outputId": "c3b59c1d-7e89-453e-9b50-c21f41b3e306"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 201, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 164, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 104, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 185, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 108, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 128, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 114, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 105, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "/home/manoj/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1281: UserWarning: Input length of input_ids is 150, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "res = []\n",
        "sequences = pipeline(\n",
        "    lst_test_data_short[750:1000],\n",
        "    max_length=100,  #200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "for ix,seq in enumerate(sequences):\n",
        "    res.append([ix, seq[0]['generated_text'].split('->:')[0] +  \"->\" +seq[0]['generated_text'].split('->:')[1]])\n",
        "\n",
        "with open('750to1000.csv', 'w') as f:\n",
        "    for i in res:\n",
        "        f.write(str(i))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9 (v3.9.9:ccb0e6a345, Nov 15 2021, 13:29:20) \n[Clang 6.0 (clang-600.0.57)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01f23cfe961646ca990bf09cf94e24ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_963e9011456a4bdcba2c1e0f62089bd0",
              "IPY_MODEL_0f287323a0344153a98637c83037a8c1",
              "IPY_MODEL_52c23e0c52fe422a81aae0c9e1542ebc"
            ],
            "layout": "IPY_MODEL_d1ce51cc3c6f490a857f47401883117c"
          }
        },
        "0f287323a0344153a98637c83037a8c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65e4ed5820f141b7836713789ac0d3d0",
            "max": 26080,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_802bf15dca8b4f69a1cbd125e0e2e586",
            "value": 26080
          }
        },
        "14da63396d4d47dfa1d796a46fdd65bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_857765cb729d436e85d29f34cdd288ce",
            "placeholder": "​",
            "style": "IPY_MODEL_a53e6cb6f3354fbd84ff498ecf385e7e",
            "value": " 8/8 [01:23&lt;00:00,  8.43s/it]"
          }
        },
        "47ac2264e3e74ca5bff02fd6e59e613c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c84cabbd43334acc8cfe492cfe6cb20c",
              "IPY_MODEL_cfb47a914f83422a96a71f725a059111",
              "IPY_MODEL_14da63396d4d47dfa1d796a46fdd65bf"
            ],
            "layout": "IPY_MODEL_9efe35cdc628424fb1eb7d916fdf6911"
          }
        },
        "52c23e0c52fe422a81aae0c9e1542ebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_838c7d9206104eebb471c4a8f72ff894",
            "placeholder": "​",
            "style": "IPY_MODEL_cf712d8d84d244dfbdeeb7d6ae6014e3",
            "value": " 26080/26080 [00:06&lt;00:00, 5145.06 examples/s]"
          }
        },
        "65e4ed5820f141b7836713789ac0d3d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "802bf15dca8b4f69a1cbd125e0e2e586": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81d1e15e894e4ddc952399e38ef41971": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82a5a9a96f63491992c83e723da52e19": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "838c7d9206104eebb471c4a8f72ff894": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "857765cb729d436e85d29f34cdd288ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d253d6a1fb842f7ac7a95b61edd4e9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "963e9011456a4bdcba2c1e0f62089bd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82a5a9a96f63491992c83e723da52e19",
            "placeholder": "​",
            "style": "IPY_MODEL_9d5096af08a54ddd9f3732ebb3a034ed",
            "value": "Map: 100%"
          }
        },
        "9d5096af08a54ddd9f3732ebb3a034ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9efe35cdc628424fb1eb7d916fdf6911": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a53e6cb6f3354fbd84ff498ecf385e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfa3092b4f0c409eb98cbccc5ef15bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c84cabbd43334acc8cfe492cfe6cb20c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d253d6a1fb842f7ac7a95b61edd4e9d",
            "placeholder": "​",
            "style": "IPY_MODEL_bfa3092b4f0c409eb98cbccc5ef15bc9",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "cf712d8d84d244dfbdeeb7d6ae6014e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfb47a914f83422a96a71f725a059111": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e879736dc46d477c8fc400e0363ab99f",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_81d1e15e894e4ddc952399e38ef41971",
            "value": 8
          }
        },
        "d1ce51cc3c6f490a857f47401883117c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e879736dc46d477c8fc400e0363ab99f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
